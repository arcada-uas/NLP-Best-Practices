{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN-MNIST.ipynb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUq2LocVwvl6LDkBNg3XC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sweetpand/NLP-Best-Practices/blob/master/DCGAN_MNIST_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDmlRI_cF6zW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import sys \n",
        "import os\n",
        "from config import Config\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMuZH1wtHob9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCGAN(object):\n",
        "    def __init__(self, input, sess, config):\n",
        "        self.input = input\n",
        "        self.X, self.y = self.load_mnist_data()\n",
        "        self.config = config\n",
        "        self.sess = sess\n",
        "        self.idx = 0\n",
        "        self.iters = 0\n",
        "        #print(self.X.shape, self.y.shape)\n",
        "    def _leakyRelu(self, input, alpha, name='leakyRelu'):\n",
        "        return tf.maximum(input, alpha * input, name=name)\n",
        "        \n",
        "    def _D_conv2d_bn(self, input, output_num, filter_size, stride, is_training=False, name=None):\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            conv = tf.contrib.layers.conv2d(input, output_num, filter_size, stride, \n",
        "                                            'SAME', activation_fn=None) \n",
        "            bn = tf.contrib.layers.batch_norm(inputs=conv, is_training=is_training, activation_fn=None)\n",
        "            return self._leakyRelu(bn, self.config.alpha)\n",
        "        \n",
        "    def _G_conv_transpose_bn(self, input, output_num, filter_size, stride, is_training, activate=True, name=None):\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            conv_transpose = tf.contrib.layers.conv2d_transpose(\n",
        "                input, output_num, filter_size, stride, 'SAME', \n",
        "                activation_fn=None, scope=name)\n",
        "            bn = tf.contrib.layers.batch_norm(inputs=conv_transpose,\n",
        "                                              is_training=is_training, \n",
        "                                              activation_fn=None, \n",
        "                                              scope=name)\n",
        "            if activate:\n",
        "                return tf.nn.relu(bn)\n",
        "            else:\n",
        "                return bn\n",
        "    \n",
        "    def generator(self, input, reuse=False, is_training=False):\n",
        "        with tf.variable_scope(\"generator\") as scope:\n",
        "            if reuse:\n",
        "                scope.reuse_variable()\n",
        "            g_0 = tf.cast(\n",
        "                tf.contrib.layers.fully_connected(input, 7 * 7 * 64, \n",
        "                                                  activation_fn=None, \n",
        "                                                  scope=\"G_linear\"), \n",
        "                dtype=tf.float32)\n",
        "            g_0 = tf.contrib.layers.batch_norm(g_0, is_training=is_training, scope=\"G_bn_0\")\n",
        "            g_0 = tf.nn.relu(g_0)\n",
        "            g_1 = tf.cast(tf.reshape(g_0, shape=[-1, 7, 7, 64]), dtype=tf.float32)\n",
        "            g_2 = self._G_conv_transpose_bn(g_1, 32, 5, 2, is_training=is_training, name=\"G_conv_trans_1\")\n",
        "            g_3 = self._G_conv_transpose_bn(g_2, 16, 5, 1, is_training=is_training, name=\"G_conv_trans_2\")\n",
        "            g_4 = self._G_conv_transpose_bn(g_3, 8, 5, 2, is_training=is_training, name=\"G_conv_trans_3\")\n",
        "            g_5 = tf.contrib.layers.conv2d_transpose(g_4, 1, 5, 1, 'SAME', activation_fn=tf.nn.tanh, scope=\"G_conv_trans_4\")\n",
        "            return g_5\n",
        "    \n",
        "    def discriminator(self, input, y=None, reuse=False, is_training=False):\n",
        "        # LeNet-like discriminator\n",
        "        with tf.variable_scope(\"discriminator\") as scope:\n",
        "            if reuse:\n",
        "                scope.reuse_variables()\n",
        "            # d_0 = tf.contrib.layers.batch_norm(input, is_training=is_training, activation_fn=None, scope=\"input\")\n",
        "            d_1 = self._D_conv2d_bn(input, 4, 5, 1, is_training, name=\"D_conv1\") # => [None, 14, 14, 8]\n",
        "            #d_1 = tf.contrib.layers.conv2d(input, 4, 5, 2, activation_fn=tf.nn.relu, scope=\"D_conv1\")\n",
        "            d_2 = self._D_conv2d_bn(d_1, 8, 5, 2, is_training, name=\"D_conv2\")  # => [None, 14, 14, 16]\n",
        "            d_3 = self._D_conv2d_bn(d_2, 32, 5, 2, is_training, name=\"D_conv3\")  # => [None, 7, 7, 32]\n",
        "            d_4 = self._D_conv2d_bn(d_3, 64, 5, 1, is_training, name=\"D_conv4\")  # => [None, 7, 7, 64]\n",
        "            d_5 = tf.contrib.layers.flatten(d_4)\n",
        "            d_6 = tf.contrib.layers.fully_connected(d_5, num_outputs=1, activation_fn=None, scope=\"D_linear\")\n",
        "            return tf.nn.sigmoid(d_6, name=\"D_output\"), d_6\n",
        "        \n",
        "    \n",
        "            \n",
        "    def build(self, config):\n",
        "        print(\"Building model...\")\n",
        "        self.is_training = tf.placeholder(bool, name=\"is_training\")\n",
        "        self.noise = tf.placeholder(shape=(None, config.latent_size),\n",
        "                                    dtype=tf.float32, name=\"noise\")\n",
        "        self.real_image = tf.placeholder(shape=(config.batch_size, 28, 28, 1), \n",
        "                                         dtype=tf.float32, name=\"real_image\")\n",
        "        \n",
        "        self._D_real, self.real_logits = self.discriminator(self.real_image,\n",
        "                                                            y=None, \n",
        "                                                            is_training=self.is_training, \n",
        "                                                            reuse=False)\n",
        "        self.fake_image = self.generator(self.noise, is_training=self.is_training)\n",
        "        \n",
        "        self._D_fake, self.fake_logits = self.discriminator(self.fake_image, \n",
        "                                                            y=None,\n",
        "                                                            is_training=self.is_training, \n",
        "                                                            reuse=True)\n",
        "        \n",
        "        self.G_loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_logits, \n",
        "                                                    labels=tf.ones_like(self._D_fake)))\n",
        "        self.D_loss_fake = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_logits, \n",
        "                                                    labels=tf.zeros_like(self._D_fake)))\n",
        "        \n",
        "        self.D_loss_real = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_logits, \n",
        "                                                    labels=tf.ones_like(self._D_real)))\n",
        "        self.D_loss = self.D_loss_real + self.D_loss_fake\n",
        "        \n",
        "        self.G_image_sum = tf.summary.image(\"G_fake_image\", self.fake_image)\n",
        "        self.G_loss_sum = tf.summary.scalar(\"G_loss\", self.G_loss)\n",
        "        \n",
        "        self.D_loss_sum = tf.summary.scalar(\"D_loss\", self.D_loss)\n",
        "        self.D_real_loss_sum = tf.summary.scalar(\"D_loss_real\", self.D_loss_real)\n",
        "        self.D_fake_loss_sum = tf.summary.scalar(\"D_loss_fake\", self.D_loss_fake)\n",
        "        \n",
        "        self.G_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\"generator\") \n",
        "        self.D_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=\"discriminator\")\n",
        "        self.updata_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        \n",
        "        self.G_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"generator\")\n",
        "        self.D_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator\")\n",
        "        self.trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "        \n",
        "        self.G_sum = tf.summary.merge([self.G_image_sum, self.G_loss_sum])\n",
        "        self.D_sum = tf.summary.merge([self.D_loss_sum, self.D_real_loss_sum, self.D_fake_loss_sum])\n",
        "        \n",
        "        self.train_summary_writer = tf.summary.FileWriter(config.summary_dir + \"/train\", self.sess.graph)\n",
        "        self.test_summary_writer = tf.summary.FileWriter(config.summary_dir + \"/test\")\n",
        "        \n",
        "    def train(self, config, is_continue=False):\n",
        "        if is_continue == False:\n",
        "            \n",
        "            self.build(config)\n",
        "            self.saver = tf.train.Saver()\n",
        "            #sess = self.sess\n",
        "            print(\"Starting training...\")\n",
        "            print(\"Model will be saved per %d epochs\" %config.save_per_epoch)\n",
        "            self.D_global_step = tf.Variable(0)\n",
        "            self.G_global_step = tf.Variable(0)\n",
        "            with tf.control_dependencies(self.D_update_ops):\n",
        "                D_opt = tf.train.AdamOptimizer(learning_rate=config.lr, beta1=config.beta1) \\\n",
        "                                .minimize(self.D_loss, var_list=self.D_trainable_vars, global_step=self.D_global_step)\n",
        "            with tf.control_dependencies(self.G_update_ops):\n",
        "                G_opt = tf.train.AdamOptimizer(learning_rate=config.lr, beta1=config.beta1) \\\n",
        "                                .minimize(self.G_loss, var_list=self.G_trainable_vars, global_step=self.G_global_step)\n",
        "            self.sess.run(tf.global_variables_initializer())\n",
        "            \n",
        "        for epoch in range(config.epoch_num):\n",
        "            batch_ix = 0\n",
        "            for X_batch, y_batch in self.minibatches(self.X, self.y, config.batch_size, True):\n",
        "                noise = np.random.uniform(-1, 1, [config.batch_size, config.latent_size])\n",
        "                feed_dict = {\n",
        "                    self.real_image: X_batch.astype(np.float32), \n",
        "                    self.noise: noise,\n",
        "                    self.is_training: True\n",
        "                }\n",
        "                \n",
        "                _, train_D_loss, D_global_step, sum_str = self.sess.run(\n",
        "                    [D_opt, self.D_loss, self.D_global_step, self.D_sum], feed_dict=feed_dict)\n",
        "                self.train_summary_writer.add_summary(sum_str, D_global_step)\n",
        "                \n",
        "                _, train_G_loss, G_global_step, sum_str = self.sess.run(\n",
        "                    [G_opt, self.G_loss, self.G_global_step, self.G_sum], feed_dict=feed_dict)\n",
        "                self.train_summary_writer.add_summary(sum_str, G_global_step)\n",
        "                \n",
        "                _, train_G_loss, G_global_step, sum_str = self.sess.run(\n",
        "                    [G_opt, self.G_loss, self.G_global_step, self.G_sum], feed_dict=feed_dict)\n",
        "                self.train_summary_writer.add_summary(sum_str, G_global_step)\n",
        "                batch_ix += 1\n",
        "                sys.stdout.write(\"Epoch %d | %d / %d batches have been trained. G_loss: %.4f, D_loss: %.4f\\r\" \n",
        "                                 % (epoch, batch_ix, 70000 // config.batch_size, train_G_loss, train_D_loss))\n",
        "                sys.stdout.flush()\n",
        "            print(\"\")\n",
        "            if (epoch  +  1) % config.save_per_epoch == 0:\n",
        "                sys.stdout.write(\"Saving model...\\r\")\n",
        "                sys.stdout.flush()\n",
        "                self.saver.save(self.sess, \"models/saved_model.ckpt\", global_step=D_global_step)\n",
        "                sys.stdout.write(\" \" * 80 + \"\\r\")\n",
        "                sys.stdout.flush()\n",
        "                print(\"Model saved at epoch %d\" % epoch)\n",
        "        print(\"\")\n",
        "        print(\"Training finished!\")    \n",
        "    def generate(self, num):\n",
        "        #sess = self.sess\n",
        "        noise = np.random.uniform(-1, 1, [num, config.latent_size])\n",
        "        fake_image = self.sess.run(self.fake_image,\n",
        "                             feed_dict={\n",
        "                                 self.noise: noise,\n",
        "                                 self.is_training: False\n",
        "                             })\n",
        "        return fake_image.reshape(num, 28, 28)\n",
        "    \n",
        "    def load_mnist_data(self, path=\"mnist.npz\"):\n",
        "        data = np.load(path)\n",
        "        X = (np.concatenate((data[\"x_train\"], data[\"x_test\"]), axis=0)) / 255.0\n",
        "        y = (np.concatenate((data[\"x_train\"], data[\"x_test\"]), axis=0)) / 255.0\n",
        "        X = np.expand_dims(X, axis=3)\n",
        "        return X, y\n",
        "    \n",
        "    def minibatches(self, inputs=None, targets=None, batch_size=None, shuffle=False):\n",
        "        assert len(inputs) == len(targets)\n",
        "        if shuffle:\n",
        "            indices = np.arange(len(inputs))\n",
        "            np.random.shuffle(indices)\n",
        "        for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
        "            if shuffle:\n",
        "                excerpt = indices[start_idx:start_idx + batch_size]\n",
        "            else:\n",
        "                excerpt = slice(start_idx, start_idx + batch_size)\n",
        "            yield inputs[excerpt], targets[excerpt]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rtQax7-HzjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_GENERATED = 9\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "\n",
        "config = Config(\n",
        "    batch_size=64, \n",
        "    latent_size=100, \n",
        "    lr=0.0002, \n",
        "    epoch_num=3, \n",
        "    beta1=0.5, \n",
        "    alpha=0.2, \n",
        "    save_per_epoch=5)\n",
        "\n",
        "test_input = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32)\n",
        "test_model = DCGAN(test_input, tf.Session(), config)\n",
        "test_model.train(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moxGwgHgH5qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated_images = test_model.generate(NUM_GENERATED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awzI0y0kH8Bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp = int(np.sqrt(NUM_GENERATED))\n",
        "f, axarr = plt.subplots(tmp, tmp, figsize=(5, 5))\n",
        "for i in range(tmp):\n",
        "    for j in range(tmp):\n",
        "        axarr[i, j].imshow(generated_images[i*tmp + j], cmap='Greys', interpolation='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3qLy03lIA4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f.savefig(\"output.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8nyF4zbIFsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}